{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "graph_mining_lab.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/djambar/courses/blob/master/graph_mining_lab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5jlpTuZyRaqn"
      },
      "source": [
        "<center><h2>DSSP<br>Lab: 27/06/2020<br>Graph Mining using Python<br></h2>G. Nikolentzos, C. Xypolopoulos, M. Vazirgiannis</center>\n",
        "\n",
        "## 1. Description\n",
        "The goal of this lab is to work with graph (or network) data using the [NexworkX](http://networkx.github.io/) library of Python. We will use this library to perform data analytics tasks on graphs. The lab is divided into three parts.\n",
        "In the first part, we will analyze a collaboration network. Then, we will implement a well-known algorithm to reveal the community structure of a simple network. Finally, we will use graph kernels to measure the similarity between graphs and to perform graph classification.\n",
        "\n",
        "## 2. Analyzing a Real-World Graph\n",
        "In the first part of the lab, we will analyze the `CA-GrQc` collaboration network, examining several structural properties. Arxiv GR-QC (General Relativity and Quantum Cosmology) collaboration network comes from the e-print arXiv and covers scientific collaborations between authors of papers submitted to General Relativity and Quantum Cosmology category. If an author $i$ co-authored a paper with another author $j$, the graph contains an undirected edge from $i$ to $j$. \n",
        "\n",
        "The graph is stored in the `CA-GrQc.txt` file as an edge list. Below you can see the first lines of the file.\n",
        "\n",
        "``` html\n",
        "# Directed graph (each unordered pair of nodes is saved once): CA-GrQc.txt \n",
        "# Collaboration network of Arxiv General Relativity category (there is an \n",
        "edge if authors coauthored at least one paper)\n",
        "# Nodes: 5242 Edges: 28980\n",
        "# FromNodeId\tToNodeId\n",
        "3466\t937\n",
        "3466\t5233\n",
        "...\n",
        "```\n",
        "\n",
        "Your first task is to load the network data into a *directed* graph $G$, using the [`read_edgelist()`](https://networkx.github.io/documentation/stable/reference/readwrite/generated/networkx.readwrite.edgelist.read_edgelist.html) function of NetworkX. Note that the delimeter used to separate values is the tab character '\\t' and additionaly, the text that follows the '#' character corresponds to comments. The general syntax of the function is the following:\n",
        "\n",
        "    G = read_edgelist(path, comments='#', delimiter='\\t', create_using=nx.Graph())\n",
        "    \n",
        "<u>Tasks</u>:\n",
        "- Load the dataset using the `read_edgelist()` function.\n",
        "- Print the number of nodes and the number of the edges of the dataset (use the [`number_of_nodes()`](https://networkx.github.io/documentation/stable/reference/classes/generated/networkx.DiGraph.number_of_nodes.html#networkx.DiGraph.number_of_nodes) and [`number_of_edges()`](https://networkx.github.io/documentation/stable/reference/classes/generated/networkx.DiGraph.number_of_edges.html#networkx.DiGraph.number_of_edges) functions of NetworkX)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nqRKLYzURaqp",
        "colab": {}
      },
      "source": [
        "import networkx as nx\n",
        "import urllib\n",
        "\n",
        "G = nx.read_edgelist(urllib.request.urlopen('http://www.db-net.aueb.gr/nikolentzos/dssp/graph_mining/data/CA-GrQc.txt'), comments='#', delimiter='\\t', create_using=nx.Graph())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Hq1y1g_4Raq2"
      },
      "source": [
        "We will next compute the number of connected components of the graph, and extract the largest connected component. A connected component is a subgraph in which any two vertices are connected to each other by paths.\n",
        "\n",
        "<u>Tasks</u>:\n",
        "- Compute the number of connected components of the graph (use the [`number_connected_components()`](https://networkx.github.io/documentation/stable/reference/algorithms/generated/networkx.algorithms.components.number_connected_components.html#networkx.algorithms.components.number_connected_components) function of NetworkX)\n",
        "- Extract the largest connected component and print its number of nodes and number of the edges (use the [`connected_components()`](https://networkx.github.io/documentation/stable/reference/algorithms/generated/networkx.algorithms.components.connected_components.html#networkx.algorithms.components.connected_components) and [`subgraph()`](https://networkx.github.io/documentation/stable/reference/classes/generated/networkx.Graph.subgraph.html) functions of NetworkX).\n",
        "- What is the proportion of nodes that belong to the largest connected component?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AUoGRxN3Raq7",
        "colab": {}
      },
      "source": [
        "# your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "T8Xp0z4mRarE"
      },
      "source": [
        "We will next analyze the degree distribution of the graph.\n",
        "\n",
        "<u>Tasks</u>:\n",
        "- Run the following code to extract the degree sequence of the graph."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "H8FGxz_zRarH",
        "colab": {}
      },
      "source": [
        "degree_sequence = list(dict(G.degree()).values())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2-6wjrhhRarO"
      },
      "source": [
        "<u>Tasks</u>:\n",
        "- Find and print the minimum, maximum, median and mean degree of the nodes of the graph (you can use the built-in functions `min, max, median` and `mean` of the `NumPy` library)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Wgnehlm4RarP",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DbKZYaUzRarX"
      },
      "source": [
        "Let's now compute and plot the degree distribution of the graph.\n",
        "\n",
        "<u>Tasks</u>:\n",
        "- Use the [`degree_histogram()`](https://networkx.github.io/documentation/stable/reference/generated/networkx.classes.function.degree_histogram.html#networkx.classes.function.degree_histogram) function of NetworkX to obtain a list of the frequency of each degree value.\n",
        "- Plot the degree histogram using the [`plot()`](http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.plot) function of matplotlib."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yORrKzG2RarZ",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QZd91mTNRarh"
      },
      "source": [
        "<u>Tasks</u>:\n",
        "- Produce again the plot using a log-log axis (use the [`loglog()`](http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.loglog) function of matplotlib)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xXcU2InBRari",
        "colab": {}
      },
      "source": [
        "# your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Qcg14qDSRaro"
      },
      "source": [
        "Next, we will compute the average clustering coefficient of the graph, which is a measure of the degree to which nodes in a graph tend to cluster together, i.e., to create tightly knit groups characterized by a relatively high density of ties. The *global clustering coefficient* is based on triplets of nodes. A triplet consists of three nodes that are connected by either two (open triplet) or three (closed triplet) undirected ties. A triangle consists of three closed triplets, one centered on each of the nodes. The global clustering coefficient is the number of closed triplets (or $3$ $\\times$ triangles) over the total number of triplets (both open and closed).\n",
        "\n",
        "<u>Tasks</u>:\n",
        "- Use the [`average_clustering()`](https://networkx.github.io/documentation/stable/reference/algorithms/generated/networkx.algorithms.cluster.average_clustering.html#networkx.algorithms.cluster.average_clustering) function of NetworkX to compute the average clustering coefficient of the graph."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4XbhWZ5jRarp",
        "colab": {}
      },
      "source": [
        "# your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KmBVVSNmRarv"
      },
      "source": [
        "Let's now repeat some parts of the above experiment for a random graph. A *random graph* is obtained by starting with a set of $n$ isolated vertices and adding successive edges between them at random. In our case, we will create a random graph using the Erdos-Renyi $G(n,p)$ random graph model, where the graph contains $n$ nodes and each of the edges is included with probability $p$.\n",
        "\n",
        "<u>Tasks</u>:\n",
        "- Create a random graph $R$ using the $G(1000, 0.1)$ model (i.e. 1000 nodes and $p=0.1$) (use the [`fast_gnp_random_graph()`](https://networkx.github.io/documentation/networkx-1.9.1/reference/generated/networkx.generators.random_graphs.fast_gnp_random_graph.html#networkx.generators.random_graphs.fast_gnp_random_graph) function of NetworkX)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AYHZPRKVRarx",
        "colab": {}
      },
      "source": [
        "# your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zNPCsNcaRar3"
      },
      "source": [
        "<u>Tasks</u>:\n",
        "- Compute again the minimum, maximum, median and mean degree of the nodes of the graph.\n",
        "- Plot the degree histogram of the graph."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o4hMZLIqRar3",
        "colab": {}
      },
      "source": [
        "# your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9BJ439LmRar-"
      },
      "source": [
        "In order to make more clear the difference of the structural properties between real-world (e.g. social networks, collaboration networks) and random networks, we can examine additional properties similar to the case of `CA-GrQc` previously. We will focus on the clustering properties, trying to stress out that random graphs do not inherently show a clustering structure.\n",
        "\n",
        "<u>Tasks</u>:\n",
        "- Compute the average clustering coefficient of the randomly generated graph."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_wOeul5JRar_",
        "colab": {}
      },
      "source": [
        "# your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oULyHPRNRasd"
      },
      "source": [
        "## 3. Community Detection\n",
        "In the second part of the lab, we will focus on the community detection (or clustering) problem in graphs. Typically, a community corresponds to a set of nodes that highly interact among each other, compared to the intensity of interactions (as expressed by the number of edges) with the rest nodes of the graph. The experiments for this part will be performed in the *karate* dataset, a small dataset that has been used as a benchmark in several community detection algorithms. This dataset is a friendship social network between 34 members of a karate club at a US university in the 1970.\n",
        "\n",
        "<u>Tasks</u>:\n",
        "- Load the graph using the NetworkX library.\n",
        "- Print the number of nodes and the number of edges of the network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4k57PuNnRasf",
        "colab": {}
      },
      "source": [
        "G = nx.read_edgelist(urllib.request.urlopen('http://www.db-net.aueb.gr/nikolentzos/dssp/graph_mining/data/karate.edgelist'), comments='#', delimiter=' ', create_using=nx.Graph())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JLrTajpFRasj"
      },
      "source": [
        "We will first implement and apply a very popular graph clustering algorithm, called Spectral Clustering. The basic idea of the algorithm is to utilize information associated to the spectrum of the graph, in order to identify well-separated clusters. The pseudocode of Spectral Clustering is shown below:\n",
        "\n",
        "1. Let $\\mathbf{A}$ be the adjacency matrix of the graph\n",
        "2. Compute the Laplacian matrix $\\mathbf{L} = \\mathbf{D}−\\mathbf{A}$. Matrix $\\mathbf{D}$ corresponds to the diagonal degree matrix of graph $G$ (i.e., degree of each node $v$ (= number of neighbors) in the main diagonal)\n",
        "3. Apply eigenvalue decomposition to the Laplacian matrix $\\mathbf{L}$ and compute the eigenvectors that correspond to $k$ smallest eigenvalues. Let $\\mathbf{U} = [\\mathbf{u}_1|\\mathbf{u}_2| \\ldots |\\mathbf{u}_k] \\in \\mathbb{R}^{n \\times k}$ be the matrix containing these eigenvectors as columns\n",
        "4. For $i = 1,\\ldots,n$, let $y_i \\in \\mathbb{R}^k$ be the vector corresponding to the $i$-th row of $\\mathbf{U}$. Apply $k$-means to the points $(y_i)_{i=1,\\ldots,n}$ (i.e., the rows of $\\mathbf{U}$) and find clusters $C_1, C_2, \\ldots, C_k$\n",
        "\n",
        "<u>Tasks</u>:\n",
        "- Implement the Spectral Clustering algorithm. To compute the Laplacian matrix $\\mathbf{L}$, you can use the [`laplacian_matrix()`](https://networkx.github.io/documentation/stable/reference/generated/networkx.linalg.laplacianmatrix.laplacian_matrix.html#networkx.linalg.laplacianmatrix.laplacian_matrix) function of NetworkX. To decompose the Laplacian matrix, you can use the [`eigs()`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.linalg.eigs.html) function of SciPy. Finally, to perform k-means, you can use scikit-learn's [implementation](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) of the algorithm. The Spectral Clustering algorithm must return a dictionary keyed by node to the cluster to which the node belongs.\n",
        "- After implementing the algorithm, apply it to the `karate` dataset, trying to identify 2 clusters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fi-lT0MeRask",
        "colab": {}
      },
      "source": [
        "from scipy.sparse.linalg import eigs\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "def spectral_clustering(G, k):\n",
        "    \n",
        "    # your code here\n",
        "    \n",
        "    return clustering\n",
        "\n",
        "# your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0Tl7nKkGRasp"
      },
      "source": [
        "We will next visualize the clustering result.\n",
        "\n",
        "<u>Task</u>:\n",
        "- Use the [`draw`](https://networkx.github.io/documentation/stable/reference/generated/networkx.drawing.nx_pylab.draw.html#networkx.drawing.nx_pylab.draw) function of NetworkX to visualize the `karate` network. Use the same color for all the nodes belonging to the same cluster."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Z2LpwzTQRasq",
        "colab": {}
      },
      "source": [
        "# your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7dDdsn9jRasv"
      },
      "source": [
        "To assess the quality of a clustering algorithm, several metrics have been proposed. Modularity is one of the most popular and widely used metrics to evaluate the quality of a network's partition into communities. Considering a specific partition of the network into clusters, modularity measures the number of edges that lie within a cluster compared to the expected number of edges of a null graph (or configuration model), i.e., a random graph with the same degree distribution. In other words, the measure of modularity is built upon the idea that random graphs are not expected to present inherent community structure; thus, comparing the observed density of a subgraph with the expected density of the same subgraph in case where edges are placed randomly, leads to a community evaluation metric. Modularity is given by the following formula:\n",
        "$$ Q = \\sum^{n_c} \\Bigg[ \\frac{l_c}{m} - \\Big(\\frac{d_c}{2m}\\Big)^2\\Bigg] $$\n",
        "where, $m=|E|$ is the total number of edges in the graph, $n_c$ is the number of communities in the graph, $l_c$ is the number of edges within the community $c$ and $d_c$ is the sum of the degrees of the nodes that belong to community $c$. Modularity takes values in the range $[-1, 1]$, with higher values indicating better community structure.\n",
        "\n",
        "<u>Task</u>:\n",
        "- Implement a function that given a graph and and an assignment of nodes to clusters, computes the modularity of the clustering result. Given a set of nodes, you can use the [`subgraph()`](https://networkx.github.io/documentation/stable/reference/classes/generated/networkx.Graph.subgraph.html) function of NetworkX to extract the subgraph induced by these nodes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "t-xKnrqTRasv",
        "colab": {}
      },
      "source": [
        "def modularity(G, clustering):\n",
        "    \n",
        "    # your code here\n",
        "    \n",
        "    return modularity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "g2k8sldvRas0"
      },
      "source": [
        "Next, we will use modularity to compare the following two clustering results of the `karate` dataset: (i) the one obtained by the Spectral Clustering algorithm, and (ii) the one obtained if we randomly partition the nodes into 2 clusters.\n",
        "To assign each node to a cluster, use the `randint(a,b)` function which returns a random integer $n$ such that $a \\leq n \\leq b$.\n",
        "\n",
        "<u>Tasks</u>:\n",
        "- Assign the nodes of the `karate` network randomly to 2 clusters to generate a random partitioning of the network.\n",
        "- Compare the two clustering results. What is the performance of the Spectral Clustering algorithm compared to the algorithm that randomly clusters the nodes?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3aR1g-Q6Ras0",
        "colab": {}
      },
      "source": [
        "from random import randint\n",
        "\n",
        "# your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Amln82qdRUS",
        "colab_type": "text"
      },
      "source": [
        "## 3. Graph Classification\n",
        "In the last part of the lab, we will focus on the problem of graph classification. Graph classification arises in the context of a number of classical domains such as chemical data, biological data, and the web. In order to perform graph classification, we will employ graph kernels, a powerful framework for graph comparison.\n",
        "\n",
        "Kernels can be intuitively understood as functions measuring the similarity of pairs of objects. More formally, for a function $k(x,x')$ to be a kernel, it has to be (1) symmetric: $k(x,x') = k(x',x)$, and (2) positive semi-definite. If a function satisfies the above two conditions on a set $\\mathcal{X}$, it is known that there exists a map $\\phi : \\mathcal{X} \\to \\mathcal{H}$ into a Hilbert space $\\mathcal{H}$, such that $k(x,x') = \\langle \\phi(x), \\phi(x') \\rangle$ for all $(x, x') \\in \\mathcal{X}^2$ where $\\langle\\cdot, \\cdot\\rangle$ is the inner product in $\\mathcal{H}$. Kernel functions thus compute the inner product between examples that are mapped in a higher-dimensional feature space. However, they do not necessarily explicitly compute the feature map $\\phi$ for each example. One advantage of kernel methods is that they can operate on very general types of data such as images and graphs. Kernels defined on graphs are known as *graph kernels*. Most graph kernels decompose graphs into their substructures and then to measure their similarity, they count the number of common substructures. Graph kernels typically focus on some structural aspect of graphs such as random walks, shortest paths, subtrees, cycles, and graphlets.\n",
        "\n",
        "We will first create a very simple graph classification dataset. The dataset will contain two types of graphs: (1) cycle graphs, and (2) path graphs. A cycle graph $C_n$ is a graph on $n$ nodes containing a single cycle through all nodes, while a path graph $P_n$ is a tree with two nodes of degree 1, and all the remaining $n-2$ nodes of degree 2. Each graph is assigned a class label: label 0 if it is a cycle or label 1 if it is a path. The Figure below illustrates such a dataset consisting of three cycle graphs and three path graphs.\n",
        "\n",
        "<img src=\"http://www.db-net.aueb.gr/nikolentzos/dssp/graph_mining/figures/synthetic_graphs.png\" width=\"500\"/>\n",
        "    \n",
        "<u>Task</u>:\n",
        "- Use the [`cycle_graph()`](https://networkx.github.io/documentation/stable/reference/generated/networkx.generators.classic.cycle_graph.html#networkx.generators.classic.cycle_graph) and [`path_graph()`](https://networkx.github.io/documentation/stable/reference/generated/networkx.generators.classic.path_graph.html#networkx.generators.classic.path_graph) functions of NetworkX to generate 100 cycle graphs and 100 path graphs of size $n=3,\\ldots,102$, respectively. Store the 200 graphs in a list $Gs$ and their class labels in another list $y$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLIP7usbdRUU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Gs = list()\n",
        "y = list()\n",
        "\n",
        "# your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGNc1dabdRUf",
        "colab_type": "text"
      },
      "source": [
        "We will next investigate if graph kernels can distinguish cycle graphs from path graphs. To this end, we will make use of the shortest path kernel, a kernel that compares shortest path lengths in two graps. Before computing the kernel, it is necessary to split the dataset into a training and a test set. We can use the [`train_test_split()`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function of scikit-learn as follows:\n",
        "    \n",
        "    from sklearn.model_selection import train_test_split\n",
        "\n",
        "    G_train, G_test, y_train, y_test = train_test_split(Gs, y, test_size=0.1)\n",
        "    \n",
        "<u>Task</u>:\n",
        "- Split the dataset into a training and a test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0n6xBaUdRUg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAQ9fB0BdRUo",
        "colab_type": "text"
      },
      "source": [
        "The shortest path kernel compares the length of shortest paths of two graphs. More specifically, given two graphs $G=(V,E)$ and $G'=(V',E')$, the shortest path kernel is defined as:\n",
        "\n",
        "$$\n",
        "k(G,G') = \\sum_{(v_1,v_2) \\in V \\times V} \\sum_{(v'_1,v'_2) \\in V' \\times V'} k_{length}(sp_{v_1,v_2}, sp_{v'_1,v'_2})\n",
        "$$\n",
        "\n",
        "where $k_{length}$ is a kernel on shortest path lengths, and $sp_{v_i,v_j}$ is the length of the shortest path between vertices $v_i$ and $v_j$. We will use the following kernel for comparing shortest path lengths:\n",
        "\n",
        "$$\n",
        "k_{length}(sp_{v_1,v_2}, sp_{v'_1,v'_2}) = \\left\\{\n",
        "            \\begin{array}{lr}\n",
        "                1 & \\text{if }sp_{v_1,v_2} = sp_{v'_1,v'_2},\\\\\n",
        "                0 & \\text{otherwise}\n",
        "            \\end{array}\n",
        "            \\right.\n",
        "$$\n",
        "\n",
        "Therefore, $k_{length}(sp_{v_1,v_2}, sp_{v'_1,v'_2})$ is equal to 1 if $sp_{v_1,v_2}$ and $sp_{v'_1,v'_2}$ are equal to each other, and $0$ otherwise.\n",
        "\n",
        "Below you are given a function that takes as input two sets of graphs (of sizes $N_1$ and $N_2$), and computes the kernel matrix $K \\in \\mathbb{R}^{N_1\\times N_2}$ which stores the kernel values between the graphs of the first set and those of the second set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30UZwm-_dRUr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sp_kernel(Gs1, Gs2):\n",
        "    N1 = len(Gs1)\n",
        "    N2 = len(Gs2)\n",
        "    \n",
        "    all_paths = dict()\n",
        "    sp_counts = dict()\n",
        "    \n",
        "    for i,G in enumerate(Gs1):\n",
        "        sp_lengths = dict(nx.shortest_path_length(G))\n",
        "        sp_counts[i] = dict()\n",
        "        nodes = G.nodes()\n",
        "        for v1 in nodes:\n",
        "            for v2 in nodes:\n",
        "                if v2 in sp_lengths[v1]:\n",
        "                    length = sp_lengths[v1][v2]\n",
        "                    if length in sp_counts[i]:\n",
        "                        sp_counts[i][length] += 1\n",
        "                    else:\n",
        "                        sp_counts[i][length] = 1\n",
        "\n",
        "                    if length not in all_paths:\n",
        "                        all_paths[length] = len(all_paths)\n",
        "                        \n",
        "    for i,G in enumerate(Gs2):\n",
        "        sp_lengths = dict(nx.shortest_path_length(G))\n",
        "        sp_counts[N1+i] = dict()\n",
        "        nodes = G.nodes()\n",
        "        for v1 in nodes:\n",
        "            for v2 in nodes:\n",
        "                if v2 in sp_lengths[v1]:\n",
        "                    length = sp_lengths[v1][v2]\n",
        "                    if length in sp_counts[N1+i]:\n",
        "                        sp_counts[N1+i][length] += 1\n",
        "                    else:\n",
        "                        sp_counts[N1+i][length] = 1\n",
        "\n",
        "                    if length not in all_paths:\n",
        "                        all_paths[length] = len(all_paths)\n",
        "\n",
        "    phi1 = np.zeros((N1, len(all_paths)))\n",
        "    for i in range(N1):\n",
        "        for length in sp_counts[i]:\n",
        "            phi1[i,all_paths[length]] = sp_counts[i][length]\n",
        "            \n",
        "    phi2 = np.zeros((N2, len(all_paths)))\n",
        "    for i in range(N2):\n",
        "        for length in sp_counts[N1+i]:\n",
        "            phi2[i,all_paths[length]] = sp_counts[N1+i][length]\n",
        "\n",
        "    K = np.dot(phi1,phi2.T)\n",
        "\n",
        "    return K"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2EX2oqsdRUx",
        "colab_type": "text"
      },
      "source": [
        "We are interested in generating two matrices. A symmetric matrix $\\mathbf{K}_{train}$ which contains the kernel values for all pairs of training graphs, and a second matrix $\\mathbf{K}_{test}$ which stores the kernel values between the graphs of the test set and those of the training set. We can obtain these two matrices very easily using the function defined above. After generating the two kernel matrices, we can use the SVM classifier to perform graph classification.\n",
        "\n",
        "<u>Tasks</u>:\n",
        "- Use the shortest path kernel to compute the $\\mathbf{K}_{train}$ and $\\mathbf{K}_{test}$ matrices.\n",
        "- Train an [SVM classifier](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) and use it to make predictions. Note that we have already pre-computed the kernel matrices (set parameter kernel equal to 'precomputed').  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dB890_bAdRUy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "# your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69kQjbCXdRU-",
        "colab_type": "text"
      },
      "source": [
        "Finally, we will evaluate the shortest path kernel. More specifically, we will compute its classification accuracy.\n",
        "\n",
        "<u>Tasks</u>:\n",
        "- Use the [`accuracy_score()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) function of scikit-learn to compute the classification accuracy of the shortest path kernel. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXfX7zXUdRU_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# your code here"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}